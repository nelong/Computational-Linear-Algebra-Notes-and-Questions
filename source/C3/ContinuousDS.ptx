<section xml:id="ContinuousDynamicalSystems">
  <title>Continuous Dynamical Systems</title>
<introduction>
<p>
  A <term>dynamical system</term> is a pair <m>(X,R)</m> where <m>X</m> is the set of states a system can be in and <m>R</m> is a rule for how the system evolves or changes. We will look at some dynamical systems where the rule of evolution will describe how the state of the system changes in terms of a continuous parameter. Let's look at some examples. 
</p>
<example>
  <p>
    When examining an electrical circuit with a resistor, capacitor, and an inductor, it is useful to look at how the current (a measure of the flow of electricity) changes as a function of time. The dynamical system in this case would consist of <m>(X,R)</m> where <m>X</m> is the set of possible functions with input <m>t</m> and <m>R</m> is the rule given by the differential equation <m>L \dfrac{d^2 I}{dt^2} + R \dfrac{d I}{dt} +\frac{1}{C} I =0</m>. In this equation the current is <m>I(t)</m> and the constants <m>R</m>, <m>L</m>, and <m>C</m> are the resistance, inductance, and capacitance of the individual components of the circuit.
  </p>
</example>
<example>
  <p>
    If you are looking at the position of an object moving under the force of gravity and under air-resistence, your dynamical system would consist of <m>(X,R)</m> where
    <ul>
      <li><m>X</m> is the set of vectors of the form <m>\vec{w}(t)=\colvec{x(t) \\y(t) \\z(t)}</m> where <m>x(t), y(t), z(t)</m> are continuous functions of <m>t</m></li>
      <li><m>R</m> is the rule of evolution given by <m> m \dfrac{d^2 w}{dt^2} = -\omega \dfrac{dw}{dt} + mg \colvec{0 \\0 \\-1}</m></li>
    </ul>
  </p>
</example>
<example>
  <title>Wave Equation</title>
  <p>
    Many different physical phenomena satisfy a very famous differential equation:
    <me>\dfrac{\partial^2 g}{\partial t^2}= c^2 (\dfrac{\partial ^2 g}{\partial x^2}+\dfrac{\partial ^2 g}{\partial y^2}+\dfrac{\partial ^2 g}{\partial z^2})</me>
    The state of the system is given by some function <m>g(x,y,z,t)</m> that may vary in both space and time coordinates. This kind of system is called a partial differential equation since there is not A derivative for a multivariable function and the change in our system depends on the various partial derivatives of our function.
  </p>
</example>
<example>
  <title>Heat Equation</title>
  <p>
    Many different physical phenomena satisfy another very famous differential equation:
    <me>\dfrac{\partial g}{\partial t}= \alpha (\dfrac{\partial ^2 g}{\partial x^2}+\dfrac{\partial ^2 g}{\partial y^2}+\dfrac{\partial ^2 g}{\partial z^2})</me>
    The state of the system is given by some function <m>g(x,y,z,t)</m> that may vary in both space and time coordinates. This is another partial differential equation.
  </p>
</example>
</introduction>
<activity>
  <introduction>
    <p>
      For this activity, we want to look at the following 2D continuous dynamical system.
      <me>\dfrac{dx}{dt}= -2x</me>
      <me>\dfrac{dy}{dt}=\frac{1}{3}y</me>
    </p>
  </introduction>
  <task>
    <p>
      What would a solution look like to this system?
    </p>
  </task>
  <task>
    <p>
      Give a solution to this system.
    </p>
  </task>
  <task>
    <p>
      Give all possible solutions to this system.
    </p>
  </task>
  <task>
    <p>
      What is the solution with <m>x(0)=1</m> and <m>y(0)=-1</m>?
    </p>
  </task>
</activity>
<p>
  None of the stuff in the previous problem seems like linear algebra, so why are we doing this stuff? The answer is that we can expand our notion of what a <q>vector</q> is and use the idea that we would like to express solutions to these systems as linear combinations of our <q>nice</q> solutions.
</p>
<subsection>
  <title>Linear Systems of Ordinary Differential Equations</title>
<p>
  In this subsection, we will look at systems of Linear ODEs of the form:
  <md>
    <mrow> \dfrac{d x_1}{dt} \amp= a_{11} x_1 \amp+ \ldots \amp+ a_{1n} x_n </mrow>
    <mrow> \dfrac{d x_2}{dt} \amp= a_{21} x_1 \amp+ \ldots \amp+ a_{2n} x_n </mrow>
    <mrow> \vdots \amp \ddots \amp  \amp \vdots </mrow>
    <mrow> \dfrac{d x_n}{dt} \amp= a_{n1} x_1 \amp+ \ldots \amp+ a_{nn} x_n </mrow>
  </md>
  with initial values given by <m>\vec{x}(0)=\vec{x}_0 = \colvec{x_1(0) \\ x_2(0) \\ \vdots \\x_n(0)}</m>.
</p>
<p>
  Notice that this system has the following properties:
  <ul>
    <li>no forcing term (the right hand side of the system does not explicitly depend on <m>t</m>)</li>
    <li>constant coefficients</li>
    <li>linear solutions (solutions are linear combinations of each other)</li>
  </ul>
</p>
<p>
  If you look around at other books and online resources, you will see that the solution to the system given by <m>\dfrac{d \vec{x}}{dt} = A \vec{x}</m> where <m>\vec{x}=\colvec{x_1(t) \\ x_2(t) \\ \vdots \\x_n(t)}</m>, will be of the form: <m>\vec{x}(t) = exp(At) \vec{x_0}</m>. The term <m>exp(At)</m> is called the matrix exponential of <m>A</m>.
</p>
<p>
  The solution to a linear continuous dynamical system involves evaluating a matrix exponential.  This is not a straightforward task and the evaluation algorithm is highly suspect in many situations.  In fact, one of the most cited papers in all of applied mathematics is written by Van Loan and Moler (founder of Matlab) titled<url ref="https://www.jstor.org/stable/2029743?seq=1#metadata_info_tab_contents">19 Dubious Ways to Compute the Exponential of a Matrix</url> written in 1978.  This paper and idea was so important in computational science and applied mathematics that it was revised by the same authors and updated 25 years later titled appropriately <url ref="https://dl.acm.org/doi/10.1137/S00361445024180" >19 Dubious Ways to Compute the Exponential of a Matrix, Twenty-Five Years Later</url>. In short, the papers primary contribution is to show that there is no accepted way to evaluate a matrix exponential for all matrices and the algorithm choice is matrix dependent. Dr. Beauregard's research takes a particular interest in symplectic approximations as they preserve fundamental physical quantities.
</p>
<p>
  Let's start with the same assumption we did for discrete dynamical systems: We will assume that we have continuous dynamical system given by a <m>n</m> by <m>n</m> matrix <m>A</m> (with rule given by <m>\dfrac{d \vec{x}}{dt} = A \vec{x}</m>) and that we can find a set of <m>n</m> linearly independent eigenvectors of <m>A</m>, <m>\{\vec{v}_1,\vec{v}_2, \ldots, \vec{v}_n\}</m>, with eigenvalues <m>\{\lambda_1,\lambda_2, \ldots, \lambda_n\}</m>. Further, let's define two matrices
  <me>
    D=diag(\lambda_1,\lambda_2, \ldots, \lambda_n)=
    \begin{bmatrix} \lambda_1 \amp 0 \amp \cdots \amp 0\\ 0 \amp \lambda_2 \amp \cdots \amp 0\\ \vdots \amp \vdots \amp \ddots \amp \vdots \\ 0 \amp 0 \amp \cdots \amp \lambda_n \end{bmatrix}
  </me>
  and <m>V=[\vec{v}_1 , \ldots , \vec{v}_n]</m>. From our work on change of coordinates, you should recognize that <m>A = V D V^{-1}</m>. So,
  <md>
    <mrow>A^k =\amp  (V D V^{-1})(V D V^{-1})\ldots (V D V^{-1})</mrow>
    <mrow> = \amp V D^k V^{-1}</mrow>
  </md>
  where <m>D^k = diag(\lambda_1^k,\lambda_2^k, \ldots, \lambda_n^k)</m>. We can use our knowledge of power series to write exponentials using
  <me>e^\alpha = \sum_{k=0}^\infty \frac{\alpha^k}{k!}</me>
  Notice that all this requires to apply to a matrix is that powers of the matrices have to make sense and the scalar multiplication by <m>\frac{1}{k!}</m> also needs to make sense. So we can define the matrix exponential as
  <me> e^{At} = V \sum_{k=0}^\infty \frac{D^k}{k!} V^{-1}= V e^{Dt} V^{-1}= V diag(e^{\lambda_1 t},e^{\lambda_2 t}, \ldots, e^{\lambda_n t})</me>
  Note here that <m>e^{At} \vec{x}_0</m> will be a vector (by matrix vector product) and thus our solution to <m>\dfrac{d \vec{x}}{dt} = A \vec{x}</m> is given by <m>\vec{x}(t) = e^{At}\vec{x}_0= V e^{Dt} V^{-1} \vec{x}_0 \: V^{-1}</m>.
</p>
<p>
  This looks a bit like our solutions to the discrete dynamical systems but still different. The vector <m>V^{-1} \vec{x}_0</m> is a solution to what matrix equation? If <m>\vec{c}=V^{-1}\vec{x}_0</m>, then <m>\vec{c}</m> is the solution to <m>V \vec{c} =\vec{x}_0</m>!!! You should recognize that <m>\vec{c}</m> is the vector of coefficients for the vector equation <m>c_1 \vec{v_1} +c_2 \vec{v}_2 +\ldots + c_n \vec{v}_n = \vec{x}_0</m>! The vector <m>\vec{c}=V^{-1}\vec{x}_0</m> comes from writing the initial condition of our system as a linear combination of the eigenvectors of <m>A</m>!
</p>
<p>
  Our solutions to <m>\dfrac{d \vec{x}}{dt} = A \vec{x}</m> are of the form
    <me>\vec{x}(t) = e^{At}\vec{x}_0= V e^{Dt} \vec{c} = \sum_{j=1}^n c_j e^{\lambda_j t} \vec{v}_j</me>
  Look at how much of this is determined by the algebra of problems you already know how to do. Which parts of this will determine the long term behavior of solutions? When will you have fixed point(s)? When will the fixed point(s) be attractors? repellors? saddles?
</p>
<example>
  <p>
    Let <m>A=\begin{bmatrix} 1.25 \amp -0.75 \\ -0.75 \amp 1.25 \end{bmatrix}</m>. As we have seen in our previous work, <m>A</m> has eigenvalues of <m>\lambda_1=1/2</m> and <m>\lambda_2=2</m> with a choice of eigenvectors given by <m>\vec{v_1}=\colvec{1\\1}</m> and <m>\vec{v}_2=\colvec{1 \\-1}</m>. The system of differential equations that corresponds to this matrix <m>A</m> is given by:
    <md>
      <mrow>\dfrac{d x_1}{dt} =\amp 1.25 x_1 -0.75 x_2 </mrow>
      <mrow>\dfrac{d x_2}{dt} =\amp -0.75 x_1 +1.25 x_2 </mrow>
    </md>
  </p>
  <p>
    Using our tools from earlier, we can see that the solutions of this system can be written in the vector form
    <md>
      <mrow>\vec{x}(t) =\amp \colvec{x_1(t)\\x_2(t)} =\amp \sum_{j=1}^n c_j e^{\lambda_j t} \vec{v}_j</mrow>
      <mrow> \amp  \amp c_1 e^{\frac{1}{2}t}\colvec{1\\1}+c_2 e^{2t} \colvec{1 \\-1}</mrow>
    </md>
  </p>
</example>
<activity>
  <p>
    To arouse a deeper interest into the study of linear systems, let us borrow from a classic example of relationships, first presented by Steven Strogatz in 1988 and then further illustrated by J.C. Sprott in 2004.
  </p>
  <p>
    Now we know the story of Romeo and Juliet.  In our situation, Romeo is desperately in love with Juliet, but Juliet is more fickle than what Shakespeare had in mind.  In fact, the more Romeo loves Juliet, the more Juliet wants to run away and hide.  This discourages Romeo and he starts to love Juliet less, strangely this is exactly the moment that Juliet finds Romeo more attractive and she begins to love him.  On the other hand, Romeo notices again that Juliet is warming up to Romeo and he begins to warm up to her as well.  Will the eager beaver (Romeo) ever find true love with the ever fickle and cautious lover (Juliet)?
  </p>
  <p>
    Let <m>R(t)</m> and <m>J(t)</m> be the love (or hate, when negative) that each person has for each other, respectively, at time <m>t</m>.  Let <m>a, b, c, d \gt 0</m>.   The love/hate relationship is governed by the dynamical system:
    <md>
      <mrow>\dfrac{dR}{dt}=\amp aR+bJ</mrow>
      <mrow>\dfrac{dJ}{dt}=\amp -cR+dJ</mrow>
    </md>
    Consider the case where <m>a=0</m>, <m>b = c = d = 1</m> and answer the following:
    <ul>
      <li>Determine the eigenvalues and eigenvectors for the coefficient matrix.</li>
      <li>Write down the general solution using the eigenvalues and eigenvectors.</li>
      <li>Use Euler's formula to simplify the result completely to determine the real-valued solution to the position function.</li>
      <li>Plot <m>R(t)</m> and <m>J(t)</m> over time.  Plot <m>R(t)</m> versus <m>J(t)</m>.  Contrast this to a phase portrait.</li>
      <li>How does the situation change if <m>a = 3</m>?  What of <m>a > 3</m>?</li>
    </ul>
  </p>
</activity>
<activity>
  <task>
    <p>
      For each of the matrices below, state the general solution for the system of differential equations given by <m>\dfrac{d\vec{x}}{dt}=A\vec{x}</m> and find the particular solution with <m>\vec{x}(1)=\colvec{-1\\2}</m>
      <ol>
        <li><m>A_1=\begin{bmatrix} 1 \amp 2 \\ 2 \amp 3 \end{bmatrix}</m></li>
        <li><m>A_2=\begin{bmatrix} 1 \amp 1 \\ -1 \amp 1 \end{bmatrix}</m></li>
        <li><m>A_3=\begin{bmatrix} 0.5 \amp 1.5 \\ 1.5 \amp 0.5 \end{bmatrix}</m></li>
        <li><m>A_4=\begin{bmatrix} 3 \amp 2 \\ 1 \amp 1 \end{bmatrix}</m></li>
      </ol>
    </p>
  </task>
</activity>

</subsection>
<subsection>
  <title>Converting Higher Order DEs to Systems</title>
<p>
  Since we have such a nice description and clean algebra to solve systems of differential equations of the form <m>\dfrac{d \vec{x}}{dt}=A \vec{x}</m>, it is common to write other types of questions in terms of a system of first order differential equations. For example, if we consider the damped harmonic oscillator (an object moving on a spring with some friction), then the system follows the differential equation
  <me>m \dfrac{d^2 x}{dt^2} = -\alpha \dfrac{d x}{dt}- k x</me>
  where <m>x(t)</m> is the function of time that measures the position of the object (as measured from the rest length of the spring), <m>m</m> is the mass of the object, <m>k</m> is the spring constant, and <m>\alpha</m> is the coefficent of friction for the object. This is called a second order differential equation because it involves a second derivative of the objective function, but we can rewrite this as first oder differential equation of <m>\vec{y}(t) =\colvec{\dfrac{dx}{dt}(t) \\ x(t)}</m>. In particular,
  <me>\dfrac{d\vec{y}}{dt}=\colvec{-\frac{\alpha}{m} \dfrac{dx}{dt}- \frac{k}{m} x \\ \dfrac{dx}{dt}}</me>
  Which takes the form <m>\dfrac{d\vec{y}}{dt}=A\vec{y}</m> for <m>A=\begin{bmatrix} -\frac{\alpha}{m} \amp - \frac{k}{m} \\ 1 \amp 0 \end{bmatrix}</m>. Thus by analyzing the second component of our solution, <m>\vec{y}(t)</m>, we can give the solution to <m>m \dfrac{d^2 x}{dt^2} = -\alpha \dfrac{d x}{dt}- k x</m>
</p>
</subsection>

</section>