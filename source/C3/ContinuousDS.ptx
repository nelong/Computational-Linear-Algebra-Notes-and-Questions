<section xml:id="ContinuousDynamicalSystems">
  <title>Continuous Dynamical Systems</title>
<p>
  A <term>dynamical system</term> is a pair <m>(X,R)</m> where <m>X</m> is the set of states a system can be in and <m>R</m> is a rule for how the system evolves or changes. We will look at some dynamical systems where the rule of evolution will describe how the state of the system changes in terms of a continuous parameter. Let's look at some examples. 
</p>
<example>
  <p>
    When examining an electrical circuit with a resistor, capacitor, and an inductor, it is useful to look at how the current (a measure of the flow of electricity) changes as a function of time. The dynamical system in this case would consist of <m>(X,R)</m> where <m>X</m> is the set of possible functions with input <m>t</m> and <m>R</m> is the rule given by the differential equation <m>L \dfrac{d^2 I}{dt^2} + R \dfrac{d I}{dt} +\frac{1}{C} I =0</m>. In this equation the current is <m>I(t)</m> and the constants <m>R</m>, <m>L</m>, and <m>C</m> are the resistance, inductance, and capacitance of the individual components of the circuit.
  </p>
</example>
<example>
  <p>
    If you are looking at the position of an object moving under the force of gravity and under air-resistence, your dynamical system would consist of <m>(X,R)</m> where
    <ul>
      <li><m>X</m> is the set of vectors of the form <m>\vec{w}(t)=\colvec{x(t) \\y(t) \\z(t)}</m> where <m>x(t), y(t), z(t)</m> are continuous functions of <m>t</m></li>
      <li><m>R</m> is the rule of evolution given by <m> m \dfrac{d^2 w}{dt^2} = -\omega \dfrac{dw}{dt} + mg \colvec{0 \\0 \\-1}</m></li>
    </ul>
  </p>
</example>
<example>
  <title>Wave Equation</title>
  <p>
    Many different physical phenomena satisfy a very famous differential equation:
    <me>\dfrac{\partial^2 g}{\partial t^2}= c^2 (\dfrac{\partial ^2 g}{\partial x^2}+\dfrac{\partial ^2 g}{\partial y^2}+\dfrac{\partial ^2 g}{\partial z^2})</me>
    The state of the system is given by some function <m>g(x,y,z,t)</m> that may vary in both space and time coordinates. This kind of system is called a partial differential equation since there is not A derivative for a multivariable function and the change in our system depends on the various partial derivatives of our function.
  </p>
</example>
<example>
  <title>Heat Equation</title>
  <p>
    Many different physical phenomena satisfy another very famous differential equation:
    <me>\dfrac{\partial g}{\partial t}= \alpha (\dfrac{\partial ^2 g}{\partial x^2}+\dfrac{\partial ^2 g}{\partial y^2}+\dfrac{\partial ^2 g}{\partial z^2})</me>
    The state of the system is given by some function <m>g(x,y,z,t)</m> that may vary in both space and time coordinates. This is another partial differential equation.
  </p>
</example>
<activity>
  <introduction>
    <p>
      For this activity, we want to look at the following 2D continuous dynamical system.
      <me>\dfrac{dx}{dt}= -2x</me>
      <me>\dfrac{dy}{dt}=\frac{1}{3}y</me>
    </p>
  </introduction>
  <task>
    <p>
      What would a solution look like to this system?
    </p>
  </task>
  <task>
    <p>
      Give a solution to this system.
    </p>
  </task>
  <task>
    <p>
      Give all possible solutions to this system.
    </p>
  </task>
  <task>
    <p>
      What is the solution with <m>x(0)=1</m> and <m>y(0)=-1</m>?
    </p>
  </task>
</activity>
<p>
  None of the stuff in the previous problem seems like linear algebra, so why are we doing this stuff? The answer is that we can expand our notion of what a <q>vector</q> is and use the idea that we would like to express solutions to these systems as linear combinations of our <q>nice</q> solutions.
</p>
<subsection>
  <title>Linear Systems of Ordinary Differential Equations</title>
<p>
  In this subsection, we will look at systems of Linear ODEs of the form:
  <md>
    <mrow> \dfrac{d x_1}{dt} \amp= a_{11} x_1 \amp+ \ldots \amp+ a_{1n} x_n </mrow>
    <mrow> \dfrac{d x_2}{dt} \amp= a_{21} x_1 \amp+ \ldots \amp+ a_{2n} x_n </mrow>
    <mrow> \vdots \amp \ddots \amp  \amp \vdots </mrow>
    <mrow> \dfrac{d x_n}{dt} \amp= a_{n1} x_1 \amp+ \ldots \amp+ a_{nn} x_n </mrow>
  </md>
  with initial values given by <m>\vec{x}(0)=\vec{x}_0 = \colvec{x_1(0) \\ x_2(0) \\ \vdots \\x_n(0)}</m>.
</p>
<p>
  Notice that this system has the following properties:
  <ul>
    <li>no forcing term (the right hand side of the system does not explicitly depend on <m>t</m>)</li>
    <li>constant coefficients</li>
    <li>linear solutions (solutions are linear combinations of each other)</li>
  </ul>
</p>
<p>
  If you look around at other books and online resources, you will see that the solution to the system given by <m>\dfrac{d \vec{x}}{dt} = A \vec{x}</m> where <m>\vec{x}=\colvec{x_1(t) \\ x_2(t) \\ \vdots \\x_n(t)}</m>, will be of the form: <m>\vec{x}(t) = exp(At) \vec{x_0}</m>. The term <m>exp(At)</m> is called the matrix exponential of <m>A</m>.
</p>
<p>
  The solution to a linear continuous dynamical system involves evaluating a matrix exponential.  This is not a straightforward task and the evaluation algorithm is highly suspect in many situations.  In fact, one of the most cited papers in all of applied mathematics is written by Van Loan and Moler (founder of Matlab) titled<url ref="https://www.jstor.org/stable/2029743?seq=1#metadata_info_tab_contents">19 Dubious Ways to Compute the Exponential of a Matrix</url> written in 1978.  This paper and idea was so important in computational science and applied mathematics that it was revised by the same authors and updated 25 years later titled appropriately <url ref="https://dl.acm.org/doi/10.1137/S00361445024180" >19 Dubious Ways to Compute the Exponential of a Matrix, Twenty-Five Years Later</url>. In short, the papers primary contribution is to show that there is no accepted way to evaluate a matrix exponential for all matrices and the algorithm choice is matrix dependent. Dr. Beauregard's research takes a particular interest in symplectic approximations as they preserve fundamental physical quantities.
</p>
<p>
  Let's start with the same assumption we did for discrete dynamical systems: We will assume that we have continuous dynamical system given by a <m>n</m> by <m>n</m> matrix <m>A</m> (with rule given by <m>\dfrac{d \vec{x}}{dt} = A \vec{x}</m>) and that we can find a set of <m>n</m> linearly independent eigenvectors of <m>A</m>, <m>\{\vec{v}_1,\vec{v}_2, \ldots, \vec{v}_n\}</m>, with eigenvalues <m>\{\lambda_1,\lambda_2, \ldots, \lambda_n\}</m>. Further, let's define two matrices
  <me>
    D=diag(\lambda_1,\lambda_2, \ldots, \lambda_n)=
    \begin{bmatrix} \lambda_1 \amp 0 \amp \cdots \amp 0\\ 0 \amp \lambda_2 \amp \cdots \amp 0\\ \vdots \amp \vdots \amp \ddots \amp \vdots \\ 0 \amp 0 \amp \cdots \amp \lambda_n \end{bmatrix}
  </me>
  and <m>V=[\vec{v}_1 , \ldots , \vec{v}_n]</m>. From our work on change of coordinates, you should recognize that <m>A = V D V^{-1}</m>. So,
  <md>
    <mrow>A^k =\amp  (V D V^{-1})(V D V^{-1})\ldots (V D V^{-1})</mrow>
    <mrow> = \amp V D^k V^{-1}</mrow>
  </md>
  where <m>D^k = diag(\lambda_1^k,\lambda_2^k, \ldots, \lambda_n^k)</m>. We can use our knowledge of power series to write exponentials using
  <me>e^\alpha = \sum_{k=0}^\infty \frac{\alpha^k}{k!}</me>
  Notice that all this requires to apply to a matrix is that powers of the matrices have to make sense and the scalar multiplication by <m>\frac{1}{k!}</m> also needs to make sense. So we can define the matrix exponential as
  <me> e^{At} = V \sum_{k=0}^\infty \frac{D^k}{k!} V^{-1}= V e^{Dt} V^{-1}= V diag(e^{\lambda_1 t},e^{\lambda_2 t}, \ldots, e^{\lambda_n t})</me>
  Note here that <m>e^{At} \vec{x}_0</m> will be a vector (by matrix vector product) and thus our solution to <m>\dfrac{d \vec{x}}{dt} = A \vec{x}</m> is given by <m>\vec{x}(t) = e^{At}\vec{x}_0= V e^{Dt} V^{-1} \vec{x}_0</m>.
</p>
<p>
  This looks a bit like our solutions to the discrete dynamical systems but still different. The vector <m>V^{-1} \vec{x}_0</m> is a solution to what matrix equation? If <m>\vec{c}=V^{-1}\vec{x}_0</m>, then <m>\vec{c}</m> is the solution to <m>V \vec{c} =\vec{x}_0</m>!!! You should recognize that <m>\vec{c}</m> is the vector of coefficients for the vector equation <m>c_1 \vec{v_1} +c_2 \vec{v}_2 +\ldots + c_n \vec{v}_n = \vec{x}_0</m>! The vector <m>\vec{c}=V^{-1}\vec{x}_0</m> comes from writing the initial condition of our system as a linear combination of the eigenvectors of <m>A</m>!
</p>
<p>
  Our solutions to <m>\dfrac{d \vec{x}}{dt} = A \vec{x}</m> are of the form
    <me>\vec{x}(t) = e^{At}\vec{x}_0= V e^{Dt} \vec{c} = \sum_{j=0}^n c_j e^{\lambda_j t} \vec{v}_j</me>
  Look at how much of this is determined by the algebra of problems you already know how to do. Which parts of this will determine the long term behavior of solutions? When will you have fixed point(s)? When will the fixed point(s) be attractors? repellors? saddles?
</p>

</subsection>
</section>